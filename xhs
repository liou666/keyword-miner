#!/usr/bin/python3
import csv
import hashlib
from urllib import parse
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import json
from lxml import html
from rich.console import Console
from rich.prompt import Prompt
import math
console = Console()



def get_x_sign(api):
    x_sign = "X"
    m = hashlib.md5()
    m.update((api + "WSUDD").encode())
    x_sign = x_sign + m.hexdigest()
    return x_sign


def spider(keyword, authorization, d_page, sort_by='general', ):
    """
    :param authorization:
    :param keyword:
    :param d_page: é¡µæ•°
    :param sort_by: generalï¼šç»¼åˆæ’åºï¼Œhot_descï¼šçƒ­åº¦æ’åº
    :return:
    """
    host = 'https://www.xiaohongshu.com'
    url = '/fe_api/burdock/weixin/v2/search/notes?keyword={}&sortBy={}' \
          '&page={}&pageSize=20&prependNoteIds=&needGifCover=true'.format(parse.quote(keyword),
                                                                          sort_by,
                                                                          d_page + 1)
    # page ä»0å¼€å§‹, æ‰€ä»¥è¿™é‡Œ+1
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36 MicroMessenger/7.0.9.501 NetType/WIFI MiniProgramEnv/Windows WindowsWechat',
        'Referer': 'https://servicewechat.com',
        'Authorization': authorization,  # åœ¨è¿™é‡Œå¡«å…¥æŠ“åˆ°çš„header
        'X-Sign': get_x_sign(url)
    }

    resp = requests.get(url=host + url, headers=headers, timeout=5)
    if resp.status_code == 200:
        res = json.loads(resp.text)
        return res['data']['notes'], res['data']['totalCount']
    else:
        print('Fail:{}'.format(resp.text))


# æ‹¿åˆ°å¸–å­ id
def getlistByName(keyword, authorization_, pageNum=1, sorted_way="general"):
    notes = []
    with console.status(f"[bold green]å¼€å§‹æ£€ç´¢å…³é”®è¯ã€{keyword}ã€‘ç›¸å…³å†…å®¹...") as status:
      for i in range(0, pageNum):
          tmp = spider(keyword, authorization_, d_page=i, sort_by=sorted_way)
          if (len(tmp[0]) <= 0):
              break
          else:
              notes.extend(tmp[0])

    ids = []
    for note in notes:
        ids.append(note['id'])

    console.print(f"âœ… æ£€ç´¢å…³é”®è¯ã€{keyword}ã€‘ç›¸å…³å†…å®¹å®Œæ¯•ï¼å…±æ£€ç´¢åˆ°" + len(ids).__str__() + "ç¯‡å†…å®¹")
    return ids

# è·å–æ–‡ç« ä¿¡æ¯è¿”å›
def getInfo(ids,keyword):
    console.print(f"ğŸ”¸ å¼€å§‹çˆ¬å–ã€{keyword}ã€‘ç›¸å…³å†…å®¹")
    infolist = []
    for id in tqdm(ids):
        url = "https://www.xiaohongshu.com/explore/" + id
        headers = {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Accept-Language": "zh-CN,zh-Hans;q=0.9",
            "Connection": "keep-alive",
            "Host": "www.xiaohongshu.com",
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_3_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.3 Mobile/15E148 Safari/604.1"

        }
        resp = requests.get(url, headers=headers)
        resp.encoding =  "utf-8"
        html = resp.text

        soup = BeautifulSoup(html, 'lxml')

        json_str = soup.find(attrs={'type': 'application/ld+json'}).text

        json_str = json_str.replace('\n', '').replace('\r\n', '')
        info_dic = json.loads(json_str, strict=False)
        info_dic['link'] = url
        
        # å°çº¢ä¹¦æœ‰åçˆ¬æœºåˆ¶çŸ­æ—¶é—´é‡å¤ ä¼šæœ‰äº›æ•°æ®è·å–ä¸åˆ° æ•°æ®ä¸ºç©ºçš„å°±è¿‡æ»¤æ‰
        if info_dic['name'] != '':
            infolist.append(info_dic)
    console.print(f"âœ… çˆ¬å–ã€{keyword}ã€‘ç›¸å…³ç¬”è®°å®Œæˆ å…±" + len(infolist).__str__() + "ç¯‡")
    return infolist


def saveCsvFile(data, keyName):
    print("ğŸ”¸ å¼€å§‹å°†æ•°æ®å†™å…¥åˆ°"+keyName+'.csvæ–‡ä»¶')

    f = open(keyName + '.csv', 'w', newline='', encoding="utf-8")
    csv_write = csv.writer(f)
    with console.status("[bold green]å¼€å§‹å°†æ•°æ®å†™å…¥åˆ°æœ¬åœ°æ–‡ä»¶...") as status:
      for i in range(len(data)):
          csv_write.writerow(data[i])
      f.close()
    console.print("âœ… æ•°æ®å·²æˆåŠŸå†™å…¥åˆ°æœ¬åœ°æ–‡ä»¶"+keyName+'.csv')


def toCsv(infolist, keyname):
    listlist = [['å°çº¢ä¹¦åœ°å€', 'æ ‡é¢˜', 'å†…å®¹', 'ä½œè€…æ˜µç§°','ä½œè€…é¦–é¡µåœ°å€']]
    for info in infolist:
        name = info['name']

        link = info['link']
        description = info['description']
        author = info['author']['name']
        authorLink=info['author']['url']
        listinfo = [link, name, description,author,authorLink]
        listlist.append(listinfo)
    saveCsvFile(listlist, keyname)


if __name__ == "__main__":
 
    keyName = Prompt.ask("[bold cyan]ğŸ“– è¾“å…¥è¦æ£€ç´¢çš„å…³é”®å­—[/bold cyan]")
    pageSize = Prompt.ask("[bold cyan]ğŸ¤”ï¸ è¾“å…¥è¦æŠ“å–çš„æ•°æ®é‡[/bold cyan]")
   
    authorization = "wxmp.28208cac-d132-46c6-8cd5-3283684bd3c5"
    sortedWay = "hot_desc"

    idList = getlistByName(keyName, authorization, math.ceil(int(pageSize)/20),sortedWay)


    toCsv(getInfo(idList,keyName), keyName)
